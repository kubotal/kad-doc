{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KAD: Kubernetes Application Deployer","text":"<p>KAD is a GitOps tool aimed to ease and automate application deployment. </p> <p>By 'application' we understand everything on top of a raw, bare Kubernetes cluster. This means not only your last nice  web-app, but also middleware and kubernetes extension such as ingress, cert-manager, any K8s operator, etc...  </p> <p>In short, anything which is installable using an Helm Chart.</p>"},{"location":"#gitops-kad-fluxcd-helm","title":"GitOps, KAD, FluxCD, Helm....","text":"<p>KAD is a GitOps engines. This means the current state of the cluster is fully described in GIT.</p> <ul> <li>When this state change in Git, the engine will reconcile the cluster state.</li> <li>There is no direct interaction with the target cluster to deploy any application.</li> <li>If a change is performed directly on the cluster, the engine will override it. (Exception can be defined) </li> </ul> <p>KAD uses FluxCD, another GitOps engine under the hood. KAD added value lies in its ability to facilitate and  industrialize the configuration of applications to be deployed. </p> <p>Most applications that can be deployed on Kubernetes come with a Helm chart. Moreover, this Helm chart is generally highly flexible, designed to accommodate as many contexts as possible. This can make its configuration quite complex.</p> <p>Furthermore, defining a FluxCD deployment (by creating a <code>HelmRelease</code> object) requires a deep understanding of the Kubernetes ecosystem. As a result, application deployment is typically the responsibility of platform administrators or platform engineers.</p> <p>And even for experienced administrators, the verbosity of Helm configurations, especially the repetition of variables, can quickly become tedious and error-prone. Therefore, industrializing these configurations is crucial to improve efficiency and reliability.</p> <p>KAD is therefore a tool that allows Platform Engineers to package applications in a way that makes it easy for less technical users (Developers, AppOps, etc.) to deploy them, by abstracting away most of the underlying technical environment details.</p> <p></p> <p>Beside user applications, these GitOps principle and tools can also by applied on system components (ingress, load balancers, K8s operators, ...).</p>"},{"location":"#what-kad-is-not","title":"What KAD is not","text":"<p>KAD is not a tool aimed to deploy Kubernetes itself, such as Kind, miniKube, Kubespray, Rancher, etc...</p> <p>KAD need at least a naked Kubernetes cluster with an API server up and running.</p>"},{"location":"#about-this-documentation","title":"About This Documentation","text":"<p>For KAD administrators, understanding its logic is essential for proper configuration.</p> <p>If, for practical reasons, you do not follow the deployment process as outlined in the Getting Started section,  it is still highly recommended to read it thoroughly. This section includes tutorials that provide a foundational understanding of the core concepts.</p>"},{"location":"todo/","title":"TODO","text":""},{"location":"getting-started/01-preparation/","title":"Preparation","text":""},{"location":"getting-started/01-preparation/#creating-the-github-repository","title":"Creating the GitHub Repository","text":"<p>The first step is to create a dedicated GIT repository to host your cluster configuration.</p> <p>Note that his repository will be capable of accommodating multiple clusters, potentially encompassing the entirety of your infrastructure.</p> <p>The easiest way to do this is to start from GitHub template we provide.</p> <p>Click on the green button located on the top right of the page: <code>Use this template</code> and <code>Create a new repository</code>.</p> <p>You can create this new repository in your personal GitHub account or within one of your organizations. It can be public or private.</p> <p>Some of the following steps may involve modifying the content of this repository</p> <ul> <li>This can be done by cloning it, editing its content, and then committing and pushing the changes.</li> <li>Alternatively, this can also be done directly on GitHub.</li> </ul>"},{"location":"getting-started/01-preparation/#installing-the-fluxcd-cli-client","title":"Installing the FluxCD CLI Client","text":"<p>You'll need to install the FluxCD client on your workstation to proceed with its installation. Please refer to the corresponding FluxCD documentation for detailed instructions.</p>"},{"location":"getting-started/01-preparation/#installation-types","title":"Installation types","text":"<p>Next, there are two variants of this installation, depending on your environment:</p> <ul> <li>On an existing cluster: This requires having an operational cluster, including an Ingress controller, suitable for this type of testing. This cluster must also have Internet access.</li> <li>From scratch: Using Kind allows you to create a minimal cluster on your workstation, which will then be enhanced by KAD with a set of extensions (Cert-manager, ingress, etc.).</li> </ul>"},{"location":"getting-started/05-installation-existing-cluster/","title":"Installation on an existing cluster","text":""},{"location":"getting-started/05-installation-existing-cluster/#prerequisites","title":"Prerequisites","text":"<p>It's assumed that you have the <code>kubectl</code> installed and that your Kubernetes client configuration (in <code>~/.kube/config</code>)  is correctly pointed at your target cluster and that you have full cluster admin rights</p> <p>Your target cluster must:</p> <ul> <li>match the FluxCD prerequisites.</li> <li>Have access to Internet</li> <li>Have an ingress controller. This tutorial has been tested with <code>NGINX ingress controller</code>, but should be  easily adapted for another one.</li> </ul> <p>You must also install the FluxCD CLI client on your workstation.</p> <p>If you do not have access to an appropriate cluster, you can perform a local deployment on your workstation, as described in the following chapter.</p>"},{"location":"getting-started/05-installation-existing-cluster/#configuration","title":"Configuration","text":"<p>Let\u2019s examine the content of the repository you created in the initial steps:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 clusters\n\u2502   \u251c\u2500\u2500 kadtest1\n\u2502   \u2502   \u251c\u2500\u2500 deployments\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 _podinfo1.yaml\n\u2502   \u2502   \u251c\u2500\u2500 flux\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 kad.yaml\n\u2502   \u2502   \u251c\u2500\u2500 system\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 .....\n\u2502   \u2502   \u2514\u2500\u2500 context.yaml\n\u2502   \u2514\u2500\u2500 kadtest2\n\u2502       \u2514\u2500\u2500 .....\n\u2514\u2500\u2500 components\n    \u251c\u2500\u2500 apps\n    \u2502   \u251c\u2500\u2500 pod-info-0.1.0.yaml\n    \u2502   \u2514\u2500\u2500 .....\n    \u2514\u2500\u2500 system\n        \u2514\u2500\u2500 .....\n</code></pre> <p>At the root level, you will find two directories:</p> <ul> <li><code>clusters</code> which will contain a subdirectory for each managed cluster.</li> <li><code>components</code> which can be understood as a library of components ready to be installed.</li> </ul> <p>In the <code>clusters</code> directory, you will find two subdirectories:</p> <ul> <li><code>kadtest1</code> which corresponds to the cluster used in this section.</li> <li><code>kadtest2</code> which corresponds to the cluster used in the second installation variant.</li> </ul> <p>The <code>kadtest1</code> directory itself contains two subdirectories and a file:</p> <ul> <li><code>deployments</code>, intended to store the definitions of the deployed applications.</li> <li><code>context.yaml</code>, a file containing all cluster's context information. More on this later.</li> <li><code>flux</code> for use by FluxCD. All Kubernetes manifests placed in this directory will be applied by FluxCD during its    initialization. Here, you will find the deployment manifest for KAD: kad.yaml.</li> <li><code>system</code>, for usage of future middleware deployments.</li> </ul> <p>File <code>clusters/kadtest1/flux/kad.yaml</code>:</p> <pre><code>--- \napiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: kad-controller\n  namespace: flux-system\nspec:\n  .....\n  values:\n    ....\n    config:\n      ....\n      primarySources:\n        - name: flux-system\n          namespace: flux-system\n          kadFiles:\n            - clusters/kadtest1/deployments\n            - clusters/kadtest1/context.yaml\n            - components\n      .....\n</code></pre> <p>The entire content of this manifest will not be detailed here. However, it is important to focus on a specific part:  <code>spec.values.config.primarySources[0].kadfiles</code>. This specifies the list of directories (or files) from the repository  root that will be taken into account by KAD. More on that later.</p> <p>If you logically wish to replace the name <code>kadtest1</code> with the actual name of your cluster, you can rename the  subdirectory under <code>clusters</code>. You must also update this name in the <code>spec.values.config.primarySources[0].kadfiles</code> list.</p> <p>Note: Don\u2019t forget to commit and push your changes if you modify the repository locally.</p>"},{"location":"getting-started/05-installation-existing-cluster/#bootstrap","title":"Bootstrap","text":"<p>The bootstrap process will modify the content of the repository. Therefore, you need to provide it with a GitHub token  that has the appropriate permissions. (You can find more detailed information on this aspect in the FluxCD documentation.)</p> <pre><code>export GITHUB_TOKEN=&lt;Your GitHub token&gt;\n</code></pre> <p>Then you can proceed withe the bootstrap.</p> <p>If the repository is in your personal GitHub account:</p> <pre><code>flux bootstrap github \\\n--owner=&lt;GitHub user&gt; \\\n--repository=&lt;Repository name&gt; \\\n--branch=main \\\n--interval 15s \\\n--read-write-key \\\n--personnal \\\n--path=clusters/kadtest1/flux\n</code></pre> <p>Or if the repository is in an organization account:</p> <pre><code>flux bootstrap github \\\n--owner=&lt;GitHub organization&gt; \\\n--repository=&lt;Repository name&gt; \\\n--branch=main \\\n--interval 15s \\\n--read-write-key \\\n--path=clusters/kadtest1/flux\n</code></pre> <p>Adjust the path to match the cluster name if you have changed it.</p> <p>The output should look like this:</p> <pre><code>\u25ba connecting to github.com\n\u25ba cloning branch \"main\" from Git repository \"https://github.com/myorga/kad-infra1.git\"\n\u2714 cloned repository\n\u25ba generating component manifests\n\u2714 generated component manifests\n\u2714 committed component manifests to \"main\" (\"d621dc6ab44f52663e1d3393df18ae14192b1888\")\n\u25ba pushing component manifests to \"https://github.com/myorga/kad-infra1.git\"\n\u25ba installing components in \"flux-system\" namespace\n......\n\u2714 public key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBHfD5hEc7Ciyth7ZB7t66dukywWFff8hakJki/C5Kf8wOOqKrO9WsOQGblRNXGmfBEtgkOrFmchIeYRLYY4CK8VjOH5rJLZK7/TziP9xM3ljUCByzgd/x28o9598Tku7gg==\n\u2714 configured deploy key \"flux-system-main-flux-system-./clusters/kadtest1/flux\" for \"https://github.com/kubotal/kad-infra-doc\"\n\u25ba applying source secret \"flux-system/flux-system\"\n....\n\u2714 all components are healthy\n</code></pre> <p>The bootstrap command above does the following:</p> <ul> <li>Adds Flux component manifests to the repository (In the <code>clusters/kadtest1/flux/flux-system</code> location)</li> <li>Deploys Flux Components to your Kubernetes Cluster.</li> <li>Create an SSH deploy key to be used by FluxCD controller to access the Git repository </li> <li>Configures Flux components to track the path <code>clusters/kadtest1/flux</code> in the repository.</li> <li>As there is a manifest <code>kad.yaml</code> in this folder, deploy the <code>kad-controller</code></li> </ul> <p>If you have cloned locally the repository, perform a <code>git pull</code> to update your local workspace.</p> <p>If installation is successful, several pods should be up and running in the <code>flux-system</code> namespace.</p> <pre><code>$ kubectl get pods -n flux-system\nNAME                                       READY   STATUS    RESTARTS   AGE\nhelm-controller-6f558f6c5d-pk5v9           1/1     Running   0          38m\nkad-controller-7c7748d5c9-spzn2            1/1     Running   0          118s\nkustomize-controller-74fb56995-dq4c8       1/1     Running   0          38m\nnotification-controller-5d794dd575-bqm8b   1/1     Running   0          38m\nsource-controller-6d597849c8-djq46         1/1     Running   0          38m\n</code></pre> <p>You can now follow up with the first deployment part</p>"},{"location":"getting-started/10-kind/","title":"Installation on kind","text":"<p>This section will guide you through creating a Kubernetes cluster on your workstation using Docker and Kind.</p> <p>Subsequently, FluxCD and KAD will be installed, which will automatically deploy middleware components such as cert-manager and ingress-nginx.</p>"},{"location":"getting-started/10-kind/#prerequisite","title":"Prerequisite:","text":"<p>The following components must be installed on your workstation:</p> <ul> <li>Docker </li> <li>Kubectl</li> <li>Kind</li> <li>Flux CLI</li> </ul> <p>The Docker daemon must be running, and you need to have an active internet connection.</p> <p>Additionally, the Kind cluster will be configured to use ports 80 and 443, so these ports must be available.</p>"},{"location":"getting-started/10-kind/#configuration","title":"Configuration","text":"<p>Let\u2019s examine the content of the repository you created in the initial steps:</p> <pre><code>\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 clusters\n\u2502   \u251c\u2500\u2500 kadtest1\n\u2502   \u2502   \u2514\u2500\u2500 .....\n\u2502   \u2514\u2500\u2500 kadtest2\n\u2502       \u251c\u2500\u2500 deployments\n\u2502       \u2502   \u2514\u2500\u2500 _podinfo1.yaml\n\u2502       \u251c\u2500\u2500 flux\n\u2502       \u2502   \u2514\u2500\u2500 kad.yaml\n\u2502       \u251c\u2500\u2500 system\n\u2502       \u2502   \u2514\u2500\u2500 .....\n\u2502       \u2514\u2500\u2500 context.yaml\n\u2514\u2500\u2500 components\n    \u251c\u2500\u2500 apps\n    \u2502   \u251c\u2500\u2500 pod-info-0.1.0.yaml\n    \u2502   \u2514\u2500\u2500 .....\n    \u2514\u2500\u2500 system\n        \u2514\u2500\u2500 .....\n</code></pre> <p>At the root level, you will find two directories:</p> <ul> <li><code>clusters</code> which will contain a subdirectory for each managed cluster.</li> <li><code>components</code> which can be understood as a library of components ready to be installed.</li> </ul> <p>In the <code>clusters</code> directory, you will find two subdirectories:</p> <ul> <li><code>kadtest1</code> which corresponds to the cluster used in the first installation variant.</li> <li><code>kadtest2</code> which corresponds to the cluster used in the this section.</li> </ul> <p>The <code>kadtest2</code> directory itself contains three subdirectories and a file:</p> <ul> <li><code>deployments</code>, intended to store the definitions of the deployed applications.</li> <li><code>system</code>, containing the middleware deployment, such as cert-manager, ingress, .... </li> <li><code>context.yaml</code>, a file containing all cluster's context information. More on this later.</li> <li><code>flux</code> for use by FluxCD. All Kubernetes manifests placed in this directory will be applied by FluxCD during its   initialization. Here, you will find the deployment manifest for KAD: <code>kad.yaml</code>.</li> </ul> <p>File <code>clusters/kadtest2/flux/kad.yaml</code>:</p> <pre><code>--- \napiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: kad-controller\n  namespace: flux-system\nspec:\n  .....\n  values:\n    ....\n    config:\n      ....\n      primarySources:\n        - name: flux-system\n          namespace: flux-system\n          kadFiles:\n            - clusters/kadtest2/deployments\n            - clusters/kadtest2/system\n            - clusters/kadtest2/context.yaml\n            - components\n      .....\n</code></pre> <p>The entire content of this manifest will not be detailed here. However, it is important to focus on a specific part: <code>spec.values.config.primarySources[0].kadfiles</code>. This specifies the list of directories (or files) from the repository root that will be taken into account by KAD. More on that later.</p>"},{"location":"getting-started/10-kind/#creating-the-cluster","title":"Creating the Cluster","text":"<p>Run the following commands to create the cluster.</p> <pre><code>cat &gt;/tmp/kadtest2-config.yaml &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: kadtest2\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 30080\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 30443\n    hostPort: 443\n    protocol: TCP\nEOF\n\nkind create cluster --config /tmp/kadtest2-config.yaml\n</code></pre> <p>These commands will create a fully operational cluster with a single node serving as both the control-plane and the worker.</p> <p>Note the <code>extraPortMapping</code> configuration, which will later allow access to the ingress-controller from your workstation once this component is deployed.</p> <p>The output should look like this:</p> <pre><code>Creating cluster \"kadtest2\" ...\n \u2713 Ensuring node image (kindest/node:v1.31.0) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-kadtest2\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kadtest2\n</code></pre> <p>Then you can check the new cluster is up and running:</p> <pre><code>$ kubectl get pods -A\nNAMESPACE            NAME                                             READY   STATUS    RESTARTS   AGE\nkube-system          coredns-6f6b679f8f-hlrms                         0/1     Pending   0          11s\nkube-system          coredns-6f6b679f8f-qw824                         0/1     Pending   0          11s\nkube-system          etcd-kadtest2-control-plane                      1/1     Running   0          18s\nkube-system          kindnet-l6z4t                                    1/1     Running   0          11s\nkube-system          kube-apiserver-kadtest2-control-plane            1/1     Running   0          17s\nkube-system          kube-controller-manager-kadtest2-control-plane   1/1     Running   0          17s\nkube-system          kube-proxy-xmm7d                                 1/1     Running   0          11s\nkube-system          kube-scheduler-kadtest2-control-plane            1/1     Running   0          17s\nlocal-path-storage   local-path-provisioner-57c5987fd4-sxsqb          0/1     Pending   0          11s\n</code></pre>"},{"location":"getting-started/10-kind/#creating-a-certificate-authority-ca","title":"Creating a Certificate Authority (CA)","text":"<p>Securing HTTP Flux communications requires the use of certificates. In the Kubernetes ecosystem, these certificates  can be automatically generated using cert-manager.</p> <p><code>cert-manager</code> requires a Certificate Authority (CA). There are several options for this:</p> <ul> <li>Use an external CA. This requires additional configuration that is outside the scope of this tutorial.</li> <li>Use a CA based on a self-signed certificate. This is the simplest solution but will only be valid for the lifetime of the cluster.</li> <li>Create a local CA that will be generated and stored locally, independently of the cluster.    This allows you to trust the CA locally on your workstation.</li> </ul> <p>This third option is described here:</p> <p>You will need the <code>openssl</code> command to create the CA.</p> <p>In a working directory, save the following script:</p> make-ca.sh <pre><code>#!/bin/bash\n\nMYDIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &amp;&amp; pwd )\"\n\n# CA name\nCA=ca\nTF=${MYDIR}/kad-ca\n\nSUBJECT=\"/C=FR/ST=Paris/L=Paris/O=Kubotal/OU=R&amp;D/CN=ca.kad.kubotal.io\"\n\nif [ -f \"${TF}/${CA}.crt\" ]\nthen\n    echo \"---------- CA already existing\"\n    exit 1\nfi\n\nmkdir -p ${TF}\n\ncat &lt;&lt; EOF &gt; ${TF}/req.cnf\n[ req ]\n#default_bits       = 2048\n#default_md     = sha256\n#default_keyfile    = privkey.pem\ndistinguished_name  = req_distinguished_name\nattributes      = req_attributes\n\n[ req_distinguished_name ]\n\n[ req_attributes ]\nchallengePassword       = A challenge password\nchallengePassword_min       = 4\nchallengePassword_max       = 20\n\n[ v3_ca ]\nbasicConstraints = critical,CA:TRUE\nsubjectKeyIdentifier = hash\nauthorityKeyIdentifier = keyid:always,issuer:always\n\nEOF\n\necho \"---------- Create CA Root Key\"\nopenssl genrsa -out ${TF}/${CA}.key 4096 2&gt;/dev/null\n\necho \"---------- Create and self sign the Root Certificate\"\nopenssl req -x509 -new -nodes -key ${TF}/${CA}.key -sha256 -days 3650 -out ${TF}/${CA}.crt -extensions v3_ca -config ${TF}/req.cnf -subj ${SUBJECT}\n\n#echo \"---------- Convert to PEM\"\n#openssl x509 -in ${TF}/${CA}.crt -out ${TF}/${CA}.pem -outform PEM\n\necho \"---------- have a look on CA:\"\nopenssl x509 -in ${TF}/${CA}.crt -text -noout | head -n 450\n</code></pre> <p>You can modify the SUBJECT variable to replace with your own attributes.</p> <p>Running this script will generate the <code>ca.crt</code> and <code>ca.key</code> files which constitute your new CA. This in a CA subfolder. </p> <p>Store these files in a secure location, as the private key must remain confidential.</p> <p>Now save and run the following script:</p> issuer-secrets.sh <pre><code>#!/bin/bash\n\nMYDIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &amp;&amp; pwd )\"\n\n# CA name\nCA=ca\nTF=${MYDIR}/kad-ca\n\nkubectl create namespace cert-manager\nkubectl create -n cert-manager secret generic cluster-issuer-kad --from-file=tls.crt=${TF}/ca.crt --from-file=tls.key=${TF}/ca.key\nkubectl create -n cert-manager secret generic cluster-issuer-kad-ca --from-file=tls.crt=${TF}/ca.crt\n</code></pre> <p>It will store the CA in two secrets that will be accessed later by cert-manager.</p>"},{"location":"getting-started/10-kind/#bootstrap","title":"Bootstrap","text":"<p>The bootstrap process will modify the content of the GIT repository. Therefore, you need to provide it with a GitHub token that has the appropriate permissions. (You can find more detailed information on this aspect in the FluxCD documentation.)</p> <pre><code>export GITHUB_TOKEN=&lt;Your GitHub token&gt;\n</code></pre> <p>Then you can proceed withe the bootstrap.</p> <p>If the repository is in your personal GitHub account:</p> <pre><code>flux bootstrap github \\\n--owner=&lt;GitHub user&gt; \\\n--repository=&lt;Repository name&gt; \\\n--branch=main \\\n--interval 15s \\\n--read-write-key \\\n--personnal \\\n--path=clusters/kadtest2/flux\n</code></pre> <p>Or if the repository is in an organization account:</p> <pre><code>flux bootstrap github \\\n--owner=&lt;GitHub organization&gt; \\\n--repository=&lt;Repository name&gt; \\\n--branch=main \\\n--interval 15s \\\n--read-write-key \\\n--path=clusters/kadtest2/flux\n</code></pre> <p>Adjust the path to match the cluster name if you have changed it.</p> <p>The output should look like this:</p> <pre><code>\u25ba connecting to github.com\n\u25ba cloning branch \"main\" from Git repository \"https://github.com/kubotal/kad-infra-doc.git\"\n\u2714 cloned repository\n\u25ba generating component manifests\n\u2714 generated component manifests\n\u2714 committed component manifests to \"main\" (\"8202c5eb6873710d725f52dfdacb0cfd3afdf787\")\n\u25ba pushing component manifests to \"https://github.com/kubotal/kad-infra-doc.git\"\n\u25ba installing components in \"flux-system\" namespace\n\u2714 installed components\n\u2714 reconciled components\n\u25ba determining if source secret \"flux-system/flux-system\" exists\n\u25ba generating source secret\n\u2714 public key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBM7kQ/I7JP79UOhXZDY4IogerKItupdzjCJl0mGCZU6OmkvZIObnlZwG+8EmNxScdXrMNYRRUIE8Bkr4Q5WGfl1itS/ziD73gTFwOWKxHYCsbb6WisqbQ6Ht5zRsa8twsA==\n.......\n\u2714 source-controller: deployment ready\n\u2714 all components are healthy\n</code></pre> <p>The bootstrap command above does the following:</p> <ul> <li>Adds Flux component manifests to the repository (In the <code>clusters/kadtest2/flux/flux-system</code> location)</li> <li>Deploys Flux Components to your Kubernetes Cluster.</li> <li>Create an SSH deploy key to be used by FluxCD controller to access the Git repository</li> <li>Configures Flux components to track the path <code>clusters/kadtest2/flux</code> in the repository.</li> <li>As there is a manifest <code>kad.yaml</code> in this folder, deploy the <code>kad-controller</code></li> <li>This <code>kad-controller</code> now track all files/folders defined in its <code>kadFiles</code> configuration.    This triggers the creation of several <code>helmReleases</code>.</li> <li>Flux CD handles the deployment of these <code>HelmReleases</code>, taking their interdependencies into account. </li> </ul> <p>If installation is successful, several new pods should be up and running:</p> <pre><code>$ kubectl get -A pods\nNAMESPACE            NAME                                             READY   STATUS    RESTARTS         AGE\ncert-manager         cert-manager-74c755695c-4qqrr                    1/1     Running   3 (3m19s ago)    2d15h\ncert-manager         cert-manager-cainjector-dcc5966bc-d6t9h          1/1     Running   10 (2m26s ago)   2d15h\ncert-manager         cert-manager-webhook-dfb76c7bd-9k8r7             1/1     Running   1 (3m19s ago)    2d15h\nflux-system          helm-controller-7f788c795c-qj5rg                 1/1     Running   12 (2m32s ago)   2d15h\nflux-system          kad-controller-76b5765554-mdh9c                  1/1     Running   6 (3m19s ago)    44h\nflux-system          kad-webserver-bb74469f4-f9l7n                    1/1     Running   6 (2m54s ago)    44h\nflux-system          kustomize-controller-b4f45fff6-8gvcw             1/1     Running   19 (2m27s ago)   2d15h\nflux-system          notification-controller-556b8867f8-fsnqr         1/1     Running   15 (2m31s ago)   2d15h\nflux-system          source-controller-77d6cd56c9-682nx               1/1     Running   31 (2m31s ago)   2d15h\ningress-nginx        ingress-nginx-controller-684d6d7756-9nrnx        1/1     Running   1 (3m19s ago)    47h\nkube-system          coredns-6f6b679f8f-n6mrf                         1/1     Running   1 (3m19s ago)    2d15h\nkube-system          coredns-6f6b679f8f-pnms7                         1/1     Running   1 (3m19s ago)    2d15h\nkube-system          etcd-kadtest2-control-plane                      1/1     Running   0                3m2s\nkube-system          kindnet-822lz                                    1/1     Running   1 (3m19s ago)    2d15h\nkube-system          kube-apiserver-kadtest2-control-plane            1/1     Running   0                3m2s\nkube-system          kube-controller-manager-kadtest2-control-plane   1/1     Running   7 (3m19s ago)    2d15h\nkube-system          kube-proxy-br722                                 1/1     Running   1 (3m19s ago)    2d15h\nkube-system          kube-scheduler-kadtest2-control-plane            1/1     Running   5 (3m19s ago)    2d15h\nlocal-path-storage   local-path-provisioner-57c5987fd4-l49wk          1/1     Running   2 (2m29s ago)    2d15h\n</code></pre> <p>Note the new pods compared to the initial state:</p> <ul> <li>The ones in <code>flux-system</code> namespace, from FluxCD and KAD</li> <li>The ones in <code>cert-manager</code> namespace</li> <li>The one in <code>ingress-nginx</code> namespace</li> </ul> <p>If you have cloned locally the repository, perform a <code>git pull</code> to update your local workspace with the modifications   performed by this process</p>"},{"location":"getting-started/10-kind/#dns-configuration","title":"DNS Configuration","text":"<p>The installation process will attach an ingress controller to ports 80 and 443 of your workstation.</p> <p>To access the URLs you will deploy later, these URLs must resolve to <code>localhost</code>. The simplest solution is to modify the  local <code>/etc/hosts</code> file as the following:</p> <pre><code>127.0.0.1   localhost podinfo1.ingress.kadtest2.k8s.local podinfo2.ingress.kadtest2.k8s.local podinfo3.ingress.kadtest2.k8s.local kad.ingress.kadtest2.k8s.local\n</code></pre> <p>These values anticipate what will be deployed later in these tutorials.</p> <p>Unfortunately, this method does not allow defining wildcard DNS entries (e.g., <code>*.ingress.kadtest2.k8s.local</code>). Installing alternatives that provide this functionality (such as <code>dnsmasq</code>) is outside the scope of this documentation.</p> <p>You can now follow up with the first deployment part</p>"},{"location":"getting-started/15-a-first-deployment/","title":"A first deployment","text":"<p>This part is relevant whatever installation type you performed. For this reason, when the cluster name is specified,  it is set as <code>kadtestX</code>, to be replaced by <code>kadtest1</code> or <code>kadtest2</code>.</p>"},{"location":"getting-started/15-a-first-deployment/#the-component-object","title":"The component object","text":"<p>In KAD, the base deployable unit is called a <code>component</code>. A <code>compoonent</code> is in fact a wrapper around an Helm Chart.</p> <p>You will find a first sample of such object in the Git repository:</p> components/apps/pod-info-0.1.0.yaml <pre><code>components:\n  - name: podinfo\n    version: 0.1.0\n    source:\n      defaultVersion: 6.7.1\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n    allowCreateNamespace: true\n    parameters:\n      ingressClassName: nginx\n      fqdn: # TBD\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Parameters.ingressClassName }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <ul> <li> <p>A component has a <code>name</code> and a <code>version</code>attributes. Note that the version of the component is not related to the version of the  referenced Helm chart.</p> </li> <li> <p>The <code>source</code> sub-element references the Helm chart that will be deployed. The version of this chart can be specified    during deployment. If not specified, the value of <code>defaultVersion</code> will be used.</p> <p>The source is of type <code>helmRepository</code>. The authors of <code>podinfo</code> provide such repository. Depending of the situation,  a source can also be a <code>gitRepository</code>, or an <code>OCIRepository</code>. More on this later in this doc. </p> </li> <li> <p>The <code>allowCreateNamespace</code> attribute allows the creation of the namespace specified during deployment, if it does not already exist.</p> </li> <li> <p>The <code>values</code> element defines a template that will be rendered to generate the <code>values.yaml</code> file used for deploying  the Helm chart.</p> <ul> <li>The templating engine used is the same as Helm's. Therefore, it will not be detailed here.</li> <li>However, the data model is different. It includes, in particular, a root object <code>.Parameters</code> that contains values    which will be defined during deployment.</li> <li>Although it may look like a <code>yaml</code> snippet, it is in fact a string, to allow insertion of template directive.</li> </ul> </li> <li> <p>The <code>parameters</code> attribute allows default values to be set to complement those provided during deployment.  It can also be used to document all the values to be supplied. (By analogy, this serves the same purpose as the  <code>values.yaml</code> file included in any well-designed Helm chart.)</p> </li> </ul> <p>This description only covers a subset of the possible attributes for a <code>component</code>.  You can find a more comprehensive description in the Guide section.</p>"},{"location":"getting-started/15-a-first-deployment/#the-componentrelease-object","title":"The componentRelease object","text":"<p>A <code>componentRelease</code> represents a deployed instance of a <code>component</code>.</p> <p>The presence of such an object triggers the creation of a Kubernetes resource of type <code>helmRelease</code>,  which will be handled by FluxCD to proceed with the deployment of the Helm chart.</p> <p>Here is a first example of the deployment of a <code>componentRelease</code>:</p> clusters/kadtestX/deployments/_podinfo1.yaml <pre><code>componentReleases:\n  - name: podinfo1\n    component:\n      name: podinfo\n      version: 0.1.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        # ingressClassName: # To be set if != nginx\n        fqdn: podinfo1.ingress.kadtestX.k8s.local # To adjust to your local context\n    namespace: podinfo1\n</code></pre> <ul> <li>The <code>name</code> of this deployment must be globally unique for a cluster. It will also be used as name for the helmRelease resource.</li> <li> <p>Next, there is the reference to the <code>component</code> being used, along with its version.</p> </li> <li> <p>The <code>component.config</code> attribute allows passing information about how the deployment will be performed by FluxCD.  Here, the namespace is created if it does not already exist.</p> <p>This can be prevented in the <code>component</code> definition, by setting <code>allowCreateNamespace</code> to false.</p> </li> <li> <p>The <code>component.parameters</code> attribute will populate the data model used to render the template  provided by the <code>values</code> attribute of the component.</p> </li> <li> <p>The <code>namespace</code> attribute specify where this component will be deployed.</p> </li> </ul>"},{"location":"getting-started/15-a-first-deployment/#making-the-deployment-effective","title":"Making the Deployment effective","text":"<p>As stated earlier, the presence of a <code>componentRelease</code> object triggers its deployment. However, in this case,  despite having a <code>componentRelease</code> named <code>podinfo1</code>, there is no corresponding active deployment in the cluster.</p> <p>The reason lies in a convention followed by KAD: files with names starting with an underscore ('_') are ignored.  For KAD, the file <code>.../deployments/_podinfo1.yaml</code> is nonexistent.</p> <p>Before proceeding with the deployment, the <code>parameters.fqdn</code> attribute must be adjusted to a valid FQDN within your context.  Additionally, the <code>parameters.ingressClassName</code> attribute may need to be updated if you are using an ingress controller  other than <code>nginx</code>.</p> <p>Once these modifications are complete, rename the file to <code>.../deployments/podinfo1.yaml</code> (removing the leading  underscore) and commit and push these changes to the GitHub repository.</p> <p>After a short period, the deployment should become effective, as shown below:</p> <pre><code>$ kubectl get pods -n podinfo1\nNAME                       READY   STATUS    RESTARTS   AGE\npodinfo1-bf57f5955-vfzxb   1/1     Running   0          37s\n</code></pre> <p>And an ingress be created:</p> <pre><code>$ kubectl get ingress -n podinfo1\nNAME       CLASS   HOSTS                                 ADDRESS         PORTS   AGE\npodinfo1   nginx   podinfo1.ingress.kadtestX.k8s.local   192.168.56.11   80      50s\n</code></pre> <p>If this is not the case, check the logs of the <code>kad-controller</code> pod in the <code>flux-system</code> namespace. More info on Debugging</p> <p>It is assumed your DNS is configured to resolve this hostname to the ingress controller entrypoint:</p> <ul> <li>In case of an existing cluster, it is assumed you can configure the DNS. Otherwise, you can use your local <code>/etc/hosts</code> file. </li> <li>In case of the Kind cluster, this configuration has been described with the cluster deployment. </li> </ul> <p>Now, pointing your browser to <code>http://podinfo1.ingress.kadtestX.k8s.local</code> should display the <code>podinfo</code> page.</p> <p>If you want to dig more on this, you can have a look of the generated FluxCD object:</p> <pre><code>$ kubectl get helmReleases -n flux-system\nNAME             AGE   READY   STATUS\nkad-controller   24h   True    Helm upgrade succeeded for release flux-system/kad-controller.v2 with chart kad-controller@0.6.0-snapshot+fa7332def1eb\npodinfo1         10h   True    Helm install succeeded for release podinfo1/podinfo1.v1 with chart podinfo@6.7.1\n</code></pre> <p>A key point of interest is the result of rendering the <code>values</code> section of the component. This rendered output becomes part of the <code>helmRelease</code>.</p> <pre><code>$ kubectl get helmReleases -n flux-system podinfo1 -o jsonpath={$.spec.values} | yq -P\ningress:\n  className: nginx\n  enabled: true\n  hosts:\n    - fqdn: podinfo1.ingress.kadtestX.k8s.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n</code></pre> <p><code>yq</code> is a small filter tool to handle json and yaml format </p> <p>For the deployment process, KAD also creates a FluxCD <code>helmRepositories</code> object. Note the name of this object, as it ensures  uniqueness when two deployments require the same helmRepository with identical characteristics.</p> <pre><code>$ kubectl get helmRepositories -n flux-system\nNAME                                             URL                                      AGE   READY   STATUS\nhttps---stefanprodan-github-io-podinfo-1h-unpr   https://stefanprodan.github.io/podinfo   10h   True    stored artifact: revision 'sha256:60fd41713cc89f0b18abc263e9c7fa9690559de220c49c1fb5b25fc3c5d3f0a6'\n</code></pre>"},{"location":"getting-started/15-a-first-deployment/#application-removal","title":"Application removal","text":"<p>If the <code>componentRelease</code> object is no longer visible to KAD, it will consider that the application should be  uninstalled and act accordingly.</p> <p>To make it invisible to KAD, you can:</p> <ul> <li>Comment out the YAML block</li> <li>Delete the file</li> <li>Rename the file with a leading _ character</li> </ul> <p>If this behavior is considered risky (for example, if the application contains persistent data), several safeguards  can be implemented. A dedicated chapter is provided to cover this aspect in detail.</p> <p>The created namespace is NOT deleted</p>"},{"location":"getting-started/20-the-deployment-process/","title":"The deployment process","text":"<p>This chapter describe how the KAD files are loaded and activated</p> <p>The starting point is the <code>kad.yaml</code> file mentioned in the installation section.</p> <pre><code>---\n.... \nspec:\n  .....\n  values:\n    ....\n    config:\n      ....\n      primarySources:\n        - name: flux-system\n          namespace: flux-system\n          kadFiles:\n            - clusters/kadtest1/deployments\n            - clusters/kadtest1/context.yaml\n            - components\n        - location: /kad-controller\n          kadFiles:\n            - tmpl            \n      .....\n</code></pre> <p>The first <code>primarySource</code>, named <code>flux-system</code> in the <code>flux-system</code> namespace, references our Git repository.</p> <p>The second <code>primarySource</code> corresponds to a local directory within the container. It contains the base templates used to generate FluxCD resources.</p> <p>The <code>kadfiles</code> entries are lists of files or directories that will be recursively explored. KAD will process all files that:</p> <ul> <li>Have a <code>.yaml</code> extension (and not <code>.yml</code>)</li> <li>Do not have names starting or ending with the character _.</li> <li>Similarly, directories with names starting or ending with _ will not be explored.</li> </ul> <p>These YAML files must be dictionaries of KAD object types, with each type containing a list of objects.</p> <p>For example, a valid file might look like this:</p> podinfo12.yaml <pre><code>componentReleases:\n  - name: podinfo1\n    component:\n      name: podinfo\n      version: 0.1.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        # ingressClassName: # To be set if != nginx\n        url: podinfo1.ingress.kadtest1.k8s.local # To adjust to your local context\n    namespace: podinfo1\n\n  - name: podinfo2\n    component:\n      name: podinfo\n      version: 0.1.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        # ingressClassName: # To be set if != nginx\n        url: podinfo2.ingress.kadtest1.k8s.local # To adjust to your local context\n    namespace: podinfo2\n\ncomponents:\n  - name: podinfo\n    version: 0.1.0\n    source:\n      defaultVersion: 6.7.1\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n    allowCreateNamespace: true\n    parameters:\n      ingressClassName: nginx\n      fqdn: # TBD\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Parameters.ingressClassName }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n</code></pre> <p>This file includes two <code>componentRelease</code> objects and the associated <code>component</code>.</p> <p>Except for the second <code>componentRelease</code> <code>'podinfo'</code>, it is equivalent to what was previously deployed.</p> <p>KAD does not impose any restrictions on how you organize your files. Similarly, the directory structure is entirely irrelevant to KAD. You can define all your objects in a single file or create one file per object. The result will be exactly the same.</p> <p>All objects collected by KAD are consolidated into a single internal repository, independent of their location in the file tree.</p> <p>One consequence of this is that, for a given object type, the name must be globally unique (or the name/version pair if the object is of a versioned type).</p> <p>Now, here is the detailed process triggered in previous example of deployment:</p> <ul> <li>The file <code>.../deployments/_podinfo1.yaml</code> was renamed to <code>.../deployments/podinfo1.yaml</code>.</li> <li>This modification was committed and pushed to the GitHub repository.</li> <li>The <code>kad-controller</code> includes a watcher on this repository, which notifies it of any changes.</li> <li>The controller performs a Git clone of the repository and rebuilds its internal referential.</li> <li>From this internal referential, it recreates all the <code>helmRelease</code> objects and applies them to Kubernetes.</li> <li>FluxCD then installs or upgrades the corresponding Helm deployments for only the <code>helmReleases</code> objects that were   effectively created or modified.</li> </ul>"},{"location":"getting-started/25-improving-component/","title":"Improving our component","text":"<p>This chapter describes an evolution of our <code>podinfo</code> component, adding support for secure connections using the  TLS protocol.</p> <p>It assumes that the cluster includes a certificate generation tool: cert-manager, providing an issuer of type <code>ClusterIssuer</code>.</p> <p>If this is not the case, we still recommend reading this chapter as it contains general information.</p> <p>If you are using the Kind cluster as described earlier, these prerequisites are already satisfied.</p>"},{"location":"getting-started/25-improving-component/#the-new-component-object","title":"The new component object","text":"<p>This new version of the component is already defined in the GIT repository, at the following location:</p> components/apps/pod-info-0.2.0.yaml <pre><code>components:\n  - name: podinfo\n    version: 0.2.0\n    source:\n      defaultVersion: 6.7.1\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n    allowCreateNamespace: true\n    parameters:\n      ingressClassName: nginx\n      fqdn: # TBD\n      tls: false\n      clusterIssuer: # TBD if tls == true\n    values: |\n      ingress:\n        enabled: true\n        className: nginx\n        {{- if .Parameters.tls }}\n        annotations:\n          cert-manager.io/cluster-issuer: {{ required \"`.Parameters.clusterIssuer` must be defined if tls: true\" .Parameters.clusterIssuer}}\n        {{- end }}\n        hosts:\n          - host: {{ .Parameters.fqdn }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n        {{- if .Parameters.tls }}\n        tls:\n          - secretName: {{ .Meta.componentRelease.name }}-tls\n            hosts:\n              - {{ .Parameters.fqdn }}\n        {{- end }}\n</code></pre> <p>The first difference is that the version number has been incremented.</p> <p>Two additional <code>parameters</code> have been added:</p> <ul> <li><code>tls</code>: A boolean to enable or disable the secure protocol.</li> <li><code>clusterIssuer</code>: The <code>ClusterIssuer</code> that will be used to generate the certificate used by the ingress controller.</li> </ul> <p>The template defined by the <code>values</code> attribute has also been updated to account for the optional TLS configuration,  adapting it to the format required by the Helm Chart of <code>podinfo</code>.</p> <p>We can also notice the appearance of a new root object: <code>.Meta</code>, which allows retrieving the deployment name.</p> <p>The goal here is to name the secret that stores the certificate. Therefore, we need a name that ensures its uniqueness.</p>"},{"location":"getting-started/25-improving-component/#the-new-componentrelease","title":"The new componentRelease","text":"<p>To deploy a component with this version, you may create a new file in Git in the deployment folder:</p> cluster/kadtestX/deployments/podinfo2.yaml <pre><code>componentReleases:\n  - name: podinfo2\n    component:\n      name: podinfo\n      version: 0.2.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        ingressClassName: # To be set if != nginx\n        fqdn: podinfo2.ingress.kadtestX.k8s.local # To adjust to your local context\n        tls: true\n        clusterIssuer: kad # To adjust to your local context\n    namespace: podinfo2\n</code></pre> <p>As usual, some parameters may need adjustment:</p> <ul> <li><code>fqdn</code>, at least to replace kadtestX</li> <li><code>clusterIssuer</code>: If you performed the deployment on kind, as stated in previous chapters, <code>kad</code> is the appropriate  value. Otherwise, the value depends of your cluster configuration.  </li> </ul> <p>After committing and pushing this addition, you should have a new pod <code>podinfo2</code> and a new <code>ingress</code> kubernetes object.</p> <p>You should be able to point your browser to <code>https://podinfo2.ingress.kadtestX.k8s.local</code> (Note the <code>https</code>).</p> <p>Of course, your DNS system must resolve <code>podinfo2.ingress.kadtestX.k8s.local</code> to your ingress-controller endpoint.</p> <p>Note than the initial deployment (<code>podinfo1</code>) is retained with its original characteristics (plain text connection).  Component versioning allows multiple versions to coexist.</p> <p>Keep in mind that the Git repository should, in principle, always reflect the current state of your cluster(s).  This state is often heterogeneous in terms of component versions.</p> <p>The principle is therefore not to modify a component that has been deployed but to create a new version in case of evolution.</p> <p>Once all associated deployments have been migrated to the new version, the initial component can be safely removed.</p> <p>As a matter of exercise, you may update the <code>podinfo1</code> <code>componentRelease</code> to change the component version number.  You will also need to set <code>tls: true</code> and the <code>clusterIssuer</code> value.</p>"},{"location":"getting-started/30-debugging/","title":"Debugging","text":"<p>What to do when things don't work as expected?</p> <p>To illustrate this scenario, an error can be triggered by commenting out the <code>component.parameters.clusterIssuer</code>  attribute of <code>podinfo2</code>, while leaving the <code>tls</code> flag set to <code>true</code>.</p> <p>Once this modification is propagated, you will notice that the READY status of the container included in the  <code>kad-controller</code> pod is set to 0.</p> <pre><code>$ kubectl get pods -n flux-system\nNAME                                       READY   STATUS    RESTARTS      AGE\nhelm-controller-6f558f6c5d-pk5v9           1/1     Running   1 (93m ago)   2d1h\nkad-controller-5459b95498-zb8rz            0/1     Running   0             19m\nkustomize-controller-74fb56995-dq4c8       1/1     Running   1 (93m ago)   2d1h\nnotification-controller-5d794dd575-bqm8b   1/1     Running   1 (93m ago)   2d1h\nsource-controller-6d597849c8-djq46         1/1     Running   1 (93m ago)   2d1h\n</code></pre> <p>Examining the logs of the kad-controller pod will reveal the following error:</p> <p><pre><code>$ kubectl logs -n flux-system kad-controller-5459b95498-zb8rz\n....\ntime=\"2024-12-20T11:36:47Z\" level=info msg=\"-- RECONCILER --\" logger=watcherReconciler object=\"flux-system:flux-system\"\ntime=\"2024-12-20T11:36:47Z\" level=info msg=\"Source #0\" logger=watcherReconciler source.location=/work/watcher/primary-sources/flux-system source.name=flux-system source.namespace=flux-system\ntime=\"2024-12-20T11:36:47Z\" level=info msg=\"Source #1 is local\" logger=watcherReconciler\ntime=\"2024-12-20T11:36:47Z\" level=info msg=Apply apiVersion=source.toolkit.fluxcd.io/v1 dryRun=false kind=HelmRepository name=https---stefanprodan-github-io-podinfo-1h-unpr namespace=flux-system\ntime=\"2024-12-20T11:36:47Z\" level=info msg=Apply apiVersion=helm.toolkit.fluxcd.io/v2 dryRun=false kind=HelmRelease name=podinfo1 namespace=flux-system\ntime=\"2024-12-20T11:36:47Z\" level=info msg=Apply apiVersion=source.toolkit.fluxcd.io/v1 dryRun=false kind=HelmRepository name=https---stefanprodan-github-io-podinfo-1h-unpr namespace=flux-system\ntime=\"2024-12-20T11:36:47Z\" level=info msg=\"Reconciliation finished with 2 error(s)\" count=2 logger=watcherReconciler\ntime=\"2024-12-20T11:36:47Z\" level=info msg=Error error=\"error on applying componentRelease 'podinfo2' componentRelease[podinfo2] (file:/work/watcher/primary-sources/flux-system/clusters/kadtest1/deployments/podinfo2.yaml, path:/): error while building model: component[podinfo:0.2.0] (file:/work/watcher/primary-sources/flux-system/components/apps/podinfo-0.2.0.yaml, path:/): error on 'values' property: error while rendering tmpl: template: :6:39: executing \\\"\\\" at &lt;required \\\"`.Parameters.clusterIssuer` must be defined if tls: true\\\" .Parameters.clusterIssuer&gt;: error calling required: `.Parameters.clusterIssuer` must be defined if tls: true\\n\" logger=watcherReconciler\ntime=\"2024-12-20T11:36:47Z\" level=info msg=Error error=\"cleaner is enabled in configuration but clenup can't be performed with errors\" logger=watcherReconciler\ntime=\"2024-12-20T11:36:54Z\" level=info msg=\"healthz check failed\" logger=controller-runtime.healthz statuses=\"[{}]\"\n</code></pre> This is an error at the KAD level. The consequence is that KAD is unable to generate a new version of the <code>helmRelease</code> object.  If it is an update, the previous version will remain untouched, and the application will remain unchanged.</p> <p>It is also possible that everything is correct at the KAD level, but the error lies within the deployment itself.</p> <p>To simulate this scenario, you can restore the definition of the <code>component.parameters.clusterIssuer</code> attribute  and comment out the definition of the <code>fqdn</code> parameter.</p> <p>After committing and pushing the changes to Git, and after a propagation delay, you will notice that there is no longer  an error in the <code>kad-controller</code> pod.</p> <pre><code>$ kubectl get pods -n flux-system\nNAME                                       READY   STATUS    RESTARTS        AGE\nhelm-controller-6f558f6c5d-pk5v9           1/1     Running   1 (3h30m ago)   2d3h\nkad-controller-5459b95498-zb8rz            1/1     Running   0               135m\nkustomize-controller-74fb56995-dq4c8       1/1     Running   1 (3h30m ago)   2d3h\nnotification-controller-5d794dd575-bqm8b   1/1     Running   1 (3h30m ago)   2d3h\nsource-controller-6d597849c8-djq46         1/1     Running   1 (3h30m ago)   2d3h\n</code></pre> <p>However, the error will now appear at the level of the corresponding helmRelease resource.</p> <pre><code>$ kubectl get helmReleases -n flux-system\nNAME             AGE    READY   STATUS\nkad-controller   2d3h   True    Helm upgrade succeeded for release flux-system/kad-controller.v3 with chart kad-controller@0.6.0-snapshot+7fb305b8c68d\npodinfo1         173m   True    Helm install succeeded for release podinfo1/podinfo1.v1 with chart podinfo@6.7.1\npodinfo2         136m   False   Helm upgrade failed for release podinfo2/podinfo2 with chart podinfo@6.7.1: cannot patch \"podinfo2\" with kind Ingress: Ingress.networking.k8s.io \"podinfo2\" is invalid: spec.tls[0].hosts[0]: Invalid value: \"\": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')\n</code></pre> <p>It would have been better to detect such errors earlier by replacing the first <code>{{.Parameters.fqdn}}</code> with <code>{{ required \"Parameters.fqdn must be defined\" .Parameters.fqdn }}</code> in the template.</p> <p>Later, the possibility of detecting such errors upstream through the use of schemas for component parameters will be discussed.</p> <p>In general, in case of malfunctions, the objects to check are:</p> <ul> <li>Ths logs of the <code>kad-controller</code> pod.</li> <li>Ths Kubernetes/FluxCD <code>HelmReleases</code> resources.</li> <li>Ths Kubernetes/FluxCD <code>HelmRepositories</code> resources.</li> <li>The Kubernetes/FluxCD <code>GitRepositories</code> resources.</li> <li>The Kubernetes/FluxCD <code>OCIRepositories</code> resources.</li> </ul>"},{"location":"getting-started/35-context/","title":"The context","text":""},{"location":"getting-started/35-context/#introduction","title":"Introduction","text":"<p>When examining the initial deployments, we can identify configuration parameters or parameter elements that are repeated.  In fact, these values are not tied to a specific deployment but are global for a given cluster.</p> <p>KAD provides a mechanism to handle this kind of global variables: the <code>context</code>.</p> <p>The <code>context</code> is a variable container. It is unique to a KAD repository, and therefore to a cluster.</p> <p>It is defined in one or more <code>kadFile</code>, as shown in the example below.</p> <pre><code>context:\n\n  ingress:\n    className: nginx\n    clusterIssuer: cluster-issuer1\n    hostPostfix: ingress.kadtest1.k8s.local\n</code></pre> <p>The <code>context:</code> entry introduces a map (or dictionary), unlike other types, which are lists.</p> <p>In this example, we find variables that are global to the cluster and common to all deployments:</p> <ul> <li>The ingress controller class (here, nginx).</li> <li>The issuer used for all ingresses.</li> <li>The common and final part of the FQDN associated with the different ingresses.</li> </ul> <p>This context will then be accessible in the data model used during the rendering of values at deployment time.</p> <p>For example, here is a new version of the <code>podinfo</code> component that will make use of it:</p> components/apps/pod-info-0.3.0.yaml <pre><code>components:\n  - name: podinfo\n    version: 0.3.0\n    source:\n      defaultVersion: 6.7.1\n      helmRepository:\n        url: https://stefanprodan.github.io/podinfo\n        chart: podinfo\n    allowCreateNamespace: true\n    parameters:\n      hostname: # TBD\n      tls: false\n    values: |\n      ingress:\n        enabled: true\n        className: {{ .Context.ingress.className }}\n        {{- if .Parameters.tls }}\n        annotations:\n          cert-manager.io/cluster-issuer: {{ required \"`.Context.ingress.clusterIssuer` must be defined if tls: true\" .Context.ingress.clusterIssuer}}\n        {{- end }}\n        hosts:\n          - host: {{ .Parameters.hostname }}.{{ .Context.ingress.hostPostfix }}\n            paths:\n              - path: /\n                pathType: ImplementationSpecific\n        {{- if .Parameters.tls }}\n        tls:\n          - secretName: {{ .Meta.componentRelease.name }}-tls\n            hosts:\n              - {{ .Parameters.hostname }}.{{ .Context.ingress.hostPostfix }}\n        {{- end }}\n</code></pre> <p>The structure of the <code>context</code> is flexible. However, it must be consistent with the various components that will use it.</p>"},{"location":"getting-started/35-context/#deployment","title":"Deployment","text":"<p>For deployment on an existing cluster, the context is defined in the file:</p> clusters/kadtest1/context.yaml <pre><code>context:\n\n  ingress:\n    className: nginx\n    clusterIssuer: cluster-issuer1\n    hostPostfix: ingress.kadtest1.k8s.local\n\n  _clusterRoles:\n    loadBalancer: true\n    ingress: true\n</code></pre> <p>And for the kind cluster:</p> clusters/kadtest1/context.yaml <pre><code>context:\n\n  ingress:\n    className: nginx\n    clusterIssuer: kad\n    hostPostfix: ingress.kadtest2.k8s.local\n\n  _clusterRoles:\n    loadBalancer: true\n</code></pre> <p>Variables starting with the character '_' are reserved by KAD.</p> <p>In this example, the <code>_clusterRoles</code> block pertains to dependency management, a topic that will be addressed later.</p> <p>It may therefore be necessary to adjust the values (especially in the case of an existing cluster). </p> <p>Then, deployment can proceed by creating a new <code>componentRelease</code> in the <code>deployment</code> folder:</p> cluster/kadtestX/deployments/podinfo3.yaml <pre><code>componentReleases:\n  - name: podinfo3\n    component:\n      name: podinfo\n      version: 0.3.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        hostname: podinfo3\n        tls: true\n    namespace: podinfo3\n</code></pre> <p>Using <code>context</code> simplifies the parameters to be provided, now reflecting only deployment-related choices rather than  infrastructure constraints.</p> <p>The <code>componentRelease</code> is now independent of the cluster configuration. This is of great value if you manage several clusters.</p>"},{"location":"getting-started/35-context/#context-construction","title":"Context Construction","text":"<p>When multiple kadFiles contain a <code>context:</code> entry, the values are aggregated, following the same logic as <code>values.yaml</code> files in Helm charts.</p> <p>The order in which the files are declared is therefore important.</p> <p>As an example, we can redefine our contexts as follows:</p> <ul> <li> <p>A file containing what is common to all clusters.</p> <p>file: <code>clusters/context.yaml</code></p> <pre><code>context:\n\n  ingress:\n    className: nginx\n    clusterIssuer: kad\n    hostPostfix: ingress.kadtest1.k8s.local\n</code></pre> </li> <li> <p>A file for the first cluster:</p> <p>file: <code>clusters/kadtest1/context.yaml</code></p> <pre><code>context:\n\n  ingress:\n    clusterIssuer: cluster-issuer1\n    hostPostfix: ingress.kadtest1.k8s.local\n</code></pre> </li> <li> <p>And another for the second one:</p> <p>file: <code>clusters/kadtest2/context.yaml</code> </p> <pre><code>context:\n\n  ingress:\n    hostPostfix: ingress.kadtest2.k8s.local\n</code></pre> </li> </ul> <p>The <code>clusterIssuer</code> of the first cluster overrides the general value.</p> <p>We removed the <code>_clusterRoles</code> block for clarity</p> <p>The configuration of the <code>kad-controller</code> will therefore integrate both the common file and the cluster specific one,  ordered from the most general to the most specific.</p> <pre><code>--- \nkind: HelmRelease\n....\nspec:\n  .....\n  values:\n    ....\n    config:\n      ....\n      primarySources:\n        - name: flux-system\n          namespace: flux-system\n          kadFiles:\n            - clusters/kadtest1/deployments\n            - components\n            - clusters/context.yaml\n            - clusters/kadtestX/context.yaml\n      .....\n</code></pre>"},{"location":"getting-started/40-kadcli/","title":"kadcli: the KAD CLI","text":""},{"location":"getting-started/40-kadcli/#server-deployment","title":"Server deployment","text":"<p>If you performed the installation from scratch using Kind, as described in a previous chapter,  the KAD server (<code>kad-webserver</code>) is already installed.</p> <p>For installation on an existing cluster, you need to activate it.  To do this, edit the following file:</p> cluster/kadtest1/system/_kad-webserver.yaml <pre><code>componentReleases:\n  - name: kad-webserver\n    enabled: true\n    component:\n      name: kad-webserver\n      version: 1.0.0\n      parameters:\n        ssl: false\n        debug: false\n        deploymentLocation: clusters/kadtest1/deployments\n        replicaCount: 1\n    namespace: flux-system\n</code></pre> <ul> <li>If you have changed the cluster name, adjust the <code>deploymentLocation</code> parameter.</li> <li>If your ingress controller supports 'SSL passthrough' mode, you can set the <code>ssl</code> flag to <code>true</code>.</li> </ul> <p>This <code>componentRelease</code> references a <code>component</code> that relies on the context defined in the previous chapter.  It is, therefore, crucial that the context is configured as previously described.</p> <p>You can find the definition of this component at the following location:</p> components/system/kad-webserver.yaml <pre><code>components:\n\n  - name: kad-webserver\n    version: 1.0.0\n    source:\n      defaultVersion: 0.6.0-snapshot\n      allowedVersions:\n        - 0.6.0-snapshot\n        - 0.6.0\n      ociRepository:\n        url: oci://quay.io/kubotal/charts/kad-webserver\n        interval: 1m\n    parameters:\n      host: # Allow override of kad.{{ .Context.ingress.hostPostfix }}\n      ssl: true\n      debug: false\n      replicaCount: 2\n      deploymentLocation: # TBD for GIT Gateway\n    catalogs:\n      - system\n    parametersSchema:\n      document: schema-parameters-kad-webserver-1.0.0\n    contextSchema:\n      document: schema-context-kad-webserver-1.0.0\n    dependsOn:\n      - ingress\n    values: |\n      replicaCount: {{ .Parameters.replicaCount }}\n      ingress:\n        {{- if .Parameters.host }}\n        host: {{ .Parameters.host }}\n        {{- else }}\n        host: kad.{{ required \".Context.ingress.hostPostfix must be defined if '.Parameters.host' is not\" .Context.ingress.hostPostfix }}\n        {{- end }}\n      webConfig:\n        server:\n          ssl: {{ .Parameters.ssl }}\n          {{- if .Parameters.ssl }}\n          certificateIssuer: {{ required \".Context.ingress.clusterIssuer must be defined if '.Parameters.ssl: true'\" .Context.ingress.clusterIssuer }}\n          {{- end }}\n        {{- with .Parameters.deploymentLocation }}\n        gitGateway:\n          deploymentLocation: {{ . }}\n        {{- end }}\n      {{- if .Parameters.debug }}\n      logger:\n        mode: dev\n        level: debug\n      image:\n        pullPolicy: Always\n      {{- end }}\n</code></pre> <p>Once these modifications are performed, you can activate the service by renaming the file <code>cluster/kadtest1/system/_kad-webserver.yaml</code> by removing the leading underscore _.</p> <p>After committing the changes to Git, you should see the corresponding pod appear:</p> <pre><code>$ kubectl get pods -n flux-system\nNAME                                       READY   STATUS    RESTARTS      AGE\nhelm-controller-6f558f6c5d-pk5v9           1/1     Running   4 (24h ago)   10d\nkad-controller-5667bc7585-tpk2g            1/1     Running   0             5m50s\nkad-webserver-6d8cdc9bb7-m4787             1/1     Running   0             5m45s\nkustomize-controller-74fb56995-dq4c8       1/1     Running   4 (24h ago)   10d\nnotification-controller-5d794dd575-bqm8b   1/1     Running   3 (24h ago)   10d\nsource-controller-6d597849c8-djq46         1/1     Running   4 (24h ago)   10d\n</code></pre>"},{"location":"getting-started/40-kadcli/#dns-configuration","title":"DNS Configuration","text":"<p>Finally, make sure your DNS is configured to resolve the host <code>kad.ingress.kadtest1.k8s.local</code> (or your modified version)  to the ingress entry point.</p> <p>For Kind clusters, this host must resolve to <code>localhost</code>. Refer to the <code>/etc/hosts</code> configuration described in the  relevant chapter.</p>"},{"location":"getting-started/40-kadcli/#client-installation","title":"Client Installation","text":"<p>The <code>kadcli</code> client is a simple binary. You only need to download the version matching your KAD installation and architecture from the following link: https://github.com/kubotal/kad-controller/releases.</p> <p>To retrieve version of the KAD server:     <pre><code>kubectl get -n flux-system deployment kad-controller -o jsonpath='{.spec.template.spec.containers[0].image}' | cut -d \":\" -f 2\n</code></pre></p> <p>Then, rename the file to <code>kadcli</code>, make it executable (<code>chmod +x kadcli</code>), and add it to your system's PATH.</p>"},{"location":"getting-started/40-kadcli/#client-usage","title":"Client Usage","text":"<p>This tool is intended for Kubernetes/KAD administrators. It assumes your Kubernetes client configuration is already  set up and grants full administrative rights.</p> <p>From this, configuration is automated. When launched, <code>kadcli</code> will access the cluster to retrieve the connection URL  and a security token stored in a kubernetes <code>secret</code> (<code>flux-system:kad-webserver-access</code>)</p> <p><code>kadcli</code> organizes its commands into three main group of subcommands:</p> <ul> <li><code>kad</code>: The primary group, allowing interaction with the KAD object repository.</li> <li><code>git</code>: Provides commands to list and modify contents in a specific Git directory.</li> <li><code>k8s</code>: Allows viewing certain Kubernetes resources.</li> </ul> <p>It is clear that the last two command groups overlap with kubectl and a Git client.  They exist only because the underlying REST APIs are designed to be used by a web front end.</p> <p>An exhaustive list of commands for each group is provided in a dedicated chapter. Below are a few examples:</p> <ul> <li> <p>Perhaps the most useful: check errors.</p> <pre><code>$ kadcli kad check\nNo errors!\n</code></pre> <p>An alternative to searching through <code>kad-controller</code> logs.</p> </li> <li> <p>Listing all defined components.</p> <pre><code>$ kadcli kad components list\nNAME           VERSION  SPD  PRTC  ERR  FILE                                                                              PATH  TITLE  CATALOGS  RELEASES\ncert-issuers   1.0.0    no   no    no   /work/webserver/primary-sources/flux-system/components/system/cert-issuers.yaml   /\ncert-manager   1.0.0    no   no    no   /work/webserver/primary-sources/flux-system/components/system/cert-manager.yaml   /\ningress-nginx  1.0.0    no   no    no   /work/webserver/primary-sources/flux-system/components/system/ingress-nginx.yaml  /\nkad-webserver  1.0.0    no   no    no   /work/webserver/primary-sources/flux-system/components/system/kad-webserver.yaml  /            system    kad-webserver\npodinfo        0.1.0    no   no    no   /work/webserver/primary-sources/flux-system/components/apps/podinfo-0.1.0.yaml    /                      podinfo1\npodinfo        0.2.0    no   no    no   /work/webserver/primary-sources/flux-system/components/apps/podinfo-0.2.0.yaml    /                      podinfo2\npodinfo        0.3.0    no   no    no   /work/webserver/primary-sources/flux-system/components/apps/podinfo-0.3.0.yaml    /                      podinfo3\n</code></pre> </li> <li> <p>Listing all active deployments.</p> <pre><code>$ kadcli kad componentReleases list\nNAME           COMPONENT            NAMESPACE    ENB.  SPD  ERR  PRTC  DEPENDENCIES  FILE                                                                                     PATH  CATALOG\nkad-webserver  kad-webserver:1.0.0  flux-system  YES   no   no   no    _CLUSTER_     /work/webserver/primary-sources/flux-system/clusters/kadtest1/system/kad-webserver.yaml  /     system\npodinfo1       podinfo:0.1.0        podinfo1     YES   no   no   no                  /work/webserver/primary-sources/flux-system/clusters/kadtest1/deployments/podinfo1.yaml  /\npodinfo2       podinfo:0.2.0        podinfo2     YES   no   no   no                  /work/webserver/primary-sources/flux-system/clusters/kadtest1/deployments/podinfo2.yaml  /\npodinfo3       podinfo:0.3.0        podinfo3     YES   no   no   no                  /work/webserver/primary-sources/flux-system/clusters/kadtest1/deployments/podinfo3.yaml  /\n</code></pre> </li> <li> <p>Inspecting the context.</p> <pre><code>$ kadcli kad context\n_clusterRoles:\n  ingress: true\n  loadBalancer: true\ningress:\n  className: nginx\n  clusterIssuer: cluster-issuer1\n  hostPostfix: ingress.kadtest1.k8s.local\n</code></pre> </li> </ul>"},{"location":"getting-started/40-kadcli/#curl-commands","title":"Curl commands","text":"<p>If you want to access this API directly, for example to build some tools around KAD, you can use the <code>--curl</code> option  on almost all commands the generate the REST API call. </p> <pre><code>$ kadcli kad components list --curl\ncurl -H \"Authorization: Bearer 247QSAj97rBWd1L1CftfVfCe8doEnqr1\" -X GET https://kad.ingress.kadtest2.k8s.local/api/kad/v1/mycluster/components\n</code></pre> <pre><code>$ curl -H \"Authorization: Bearer 247QSAj97rBWd1L1CftfVfCe8doEnqr1\" -X GET https://kad.ingress.kadtest2.k8s.local/api/kad/v1/mycluster/components\n[{\"kind\":\"component\",\"spec\":{\"name\":\"ingress-nginx\",\"version\":\"1.0.0\",\"catalogs\":null,\"usage\":{\"text\":\"\",\"file\":\"\",\"document\":\"\"},\"tmpl__\":{\"Name\":\"helmRelease\",\"Version\":\"1.0.0\"}...........\n</code></pre>"},{"location":"getting-started/45-repository-source/","title":"OCI and GIT repository source","text":"<p>We will now proceed with the deployment of two interconnected applications:</p> <ul> <li>Redis</li> <li>Redis Commander, a frontend interface for accessing Redis.</li> </ul>"},{"location":"getting-started/45-repository-source/#redis-oci-repository","title":"Redis: OCI Repository","text":"<p>For the Redis deployment, we use the chart provided by Bitnami. This chart is packaged as an OCI image.</p> <p>This packaging format for Helm charts is becoming increasingly common due to its flexibility and its reliance on the same infrastructure as container images.</p> <p>Here is the component using This Chart:</p> components/middleware/redis-0.1.0.yaml <pre><code>components:\n  - name: redis\n    version: 0.1.0\n    source:\n      defaultVersion: 20.6.1\n      ociRepository:\n        url: oci://registry-1.docker.io/bitnamicharts/redis\n    allowCreateNamespace: true\n    parameters:\n      password: # TBD\n      replicaCount: 1\n    values: |\n      fullnameOverride: {{ .Meta.componentRelease.name }}\n      global:\n        redis:\n          password: {{ .Parameters.password }}\n      master:\n        persistence:\n          enabled: false\n      replica:\n        persistence:\n          enabled: false\n        replicaCount: {{ .Parameters.replicaCount }}      \n</code></pre> <p>Key notes:</p> <ul> <li>The source type is <code>ociRepository</code>. The only parameters required are the image URL and its version (used as <code>tag</code>).</li> <li>For simplicity in this example, Redis persistence is disabled.</li> <li>The <code>fullnameOverride</code> variable in the <code>values</code> block defines a prefix for naming all deployed resources.    By default, it combines the chart name and release name. To ensure predictability, it is overridden    with the name of our deployment.</li> </ul>"},{"location":"getting-started/45-repository-source/#redis-commander-git-repository","title":"Redis Commander: Git Repository","text":"<p>In addition to Redis, we want to deploy the Redis Commander tool.</p> <p>The authors of Redis Commander provide a Helm chart in their GitHub repository. However, this chart is not published as an OCI image or in a Helm repository.</p> <p>Fortunately, FluxCD and KAD allow us to use a Helm chart directly from a Git repository. To achieve this, we need to use a <code>gitRepository</code> KAD object.</p> <p>This object, along with the redis-commander component, can be found in the following file:</p> components/middleware/redis-commander-0.1.0.yaml <pre><code>gitRepositories:\n\n  - name: redis-commander\n    ref:\n      branch: master\n    url: https://github.com/joeferner/redis-commander.git\n    ignore: |\n      # exclude all\n      /*\n      # include helm chart dir\n      !/k8s/helm-chart/redis-commander\n\ncomponents:\n\n- name: redis-commander\n  version: 0.1.0\n  source:\n    defaultVersion: 0.6.0\n    gitRepository:\n      name: redis-commander\n      path: k8s/helm-chart/redis-commander\n  parameters:\n    redis:\n      password: # TBD\n      host: redis-master\n    tls: false\n    hostname: # TBD\n  values: |\n    redis:\n      host: {{ .Parameters.redis.host }}\n      password: {{ .Parameters.redis.password }}\n    ingress:\n      enabled: true\n      className: {{ .Context.ingress.className }}\n      {{- if .Parameters.tls }}\n      annotations:\n        cert-manager.io/cluster-issuer: {{ required \"`.Context.ingress.clusterIssuer` must be defined if tls: true\" .Context.ingress.clusterIssuer}}\n      {{- end }}\n      hosts:\n        - host: {{ .Parameters.hostname }}.{{ .Context.ingress.hostPostfix }}\n          paths:\n            - \"/\"\n      {{- if .Parameters.tls }}\n      tls:\n        - secretName: {{ .Meta.componentRelease.name }}-tls\n          hosts:\n            - {{ .Parameters.hostname }}.{{ .Context.ingress.hostPostfix }}\n      {{- end }}\n</code></pre> <p>Key notes:</p> <ul> <li>The <code>ref</code> sub-element can refer to a branch (as in this example), a <code>tag</code>, a <code>commit</code> ID, a <code>semver</code>, or a <code>name</code>.    See the FluxCD documentation for details.</li> <li>FluxCD archives the repository's content locally. To reduce the load, this archiving can be restricted to the    necessary files using the <code>ignore</code> attribute, which uses <code>.gitignore</code>-style syntax.</li> <li>The component leverages the <code>context</code> defined earlier. It must be properly set.</li> </ul>"},{"location":"getting-started/45-repository-source/#the-fluxcd-gitrepository-object","title":"The FluxCD <code>gitRepository</code> Object","text":"<p>Examining the list of <code>gitRepository</code> resources:</p> <pre><code>$ kubectl -n flux-system get gitRepository\nNAME              URL                                                AGE   READY   STATUS\nflux-system       ssh://git@github.com/kubotal/kad-infra-doc         15d   True    stored artifact for revision 'main@sha1:e8788571eb8c2a7f642e0c90293a3ef8e3f3feb1'\nredis-commander   https://github.com/joeferner/redis-commander.git   12h   True    stored artifact for revision 'master@sha1:a92d4d2424e527aee938af98448686c205e3df45'\n</code></pre> <p>We find:</p> <ul> <li>Our redis-commander repository.</li> <li>The initial Git repository created during the Flux bootstrap process.</li> </ul>"},{"location":"getting-started/45-repository-source/#deployment","title":"Deployment","text":"<p>Here is a first deployment of the stack:</p> storehouse/redis-stack-1.yaml <pre><code>componentReleases:\n\n  - name: redis1-redis\n    component:\n      name: redis\n      version: 0.1.0\n      config:\n        install:\n          createNamespace: true\n      parameters:\n        password: admin123\n        replicaCount: 1\n    namespace: redis1\n\n  - name: redis1-commander\n    component:\n      name: redis-commander\n      version: 0.1.0\n      parameters:\n        redis:\n          host: redis1-redis-master\n          password: admin123\n        hostname: redis1\n        tls: true\n    namespace: redis1\n</code></pre> <p>Key notes:</p> <ul> <li> <p>The <code>password</code> secures communication between Redis and Redis Commander. It must be identical on both sides.</p> <p>Providing the password as a parameter means it will be stored in Git, which is not acceptable in a production  environment. But, implementing a secure solution is outside the scope of this document.</p> </li> <li> <p>The <code>redis.host</code> parameter in Redis Commander corresponds to the kubernetes <code>service</code> associated with the Redis master.</p> </li> <li>The <code>hostname</code> parameter is the access prefix for our instance. Therefore, we will be able to connect to the URL    <code>redis1.ingress.kadtestX.k8s.local</code>, provided that the name is correctly configured in the DNS being used.</li> </ul> <p>This deployment file is located in the <code>storehouse</code> directory, which is NOT included in the <code>kadFiles</code> list defined  in the KAD configuration. As a result, it is ignored by KAD</p> <p>To proceed with the actual deployment, simply copy this file into the deployment directory of your cluster  (<code>clusters/kadTestX/deployments</code>), where it will be automatically processed by KAD.</p> <p>Note that since the components use the <code>context</code>, this deployment is independent of the target cluster (But your  <code>context</code> must be properly set, as defined in previous chapter)</p> <p>If everything goes well, three new <code>pods</code> should be created:</p> <pre><code>$ kubectl -n redis1 get pods\nNAME                                                READY   STATUS    RESTARTS      AGE\nredis1-commander-redis-commander-68f8dfcd75-xvsj2   1/1     Running   0             37h\nredis1-redis-master-0                               1/1     Running   0             37h\nredis1-redis-replicas-0                             1/1     Running   2 (15h ago)   37h\n</code></pre> <p>As well as an <code>ingress</code> allowing access to Redis Commander:</p> <pre><code>$ kubectl -n redis1 get ingress\nNAME                               CLASS   HOSTS                               ADDRESS         PORTS     AGE\nredis1-commander-redis-commander   nginx   redis1.ingress.kadtest1.k8s.local   192.168.56.11   80, 443   37h\n</code></pre> <p>Don't forget to configure the corresponding DNS entry.</p> <p>We can also view the Kubernetes <code>services</code> created for Redis, with <code>redis1-redis-master</code> being the one used by Redis Commander.</p> <pre><code>$ kubectl -n redis1 get services\nNAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nredis1-commander-redis-commander   ClusterIP   10.233.15.94    &lt;none&gt;        80/TCP     37h\nredis1-redis-headless              ClusterIP   None            &lt;none&gt;        6379/TCP   37h\nredis1-redis-master                ClusterIP   10.233.14.140   &lt;none&gt;        6379/TCP   37h\nredis1-redis-replicas              ClusterIP   10.233.14.179   &lt;none&gt;        6379/TCP   37h\n</code></pre>"},{"location":"getting-started/45-repository-source/#the-namespace-creation","title":"The namespace Creation","text":"<p>The namespace for these components is created by the first component deployed. However, there is no guarantee regarding the creation order.</p> <p>Additionally, namespaces created as a side effect of deploying a Helm chart cannot be customized (e.g., adding <code>labels</code> or <code>annotations</code>).</p> <p>A best practice for deploying an application stack is to explicitly create the namespace with desired characteristics.  This approach also allows adding resources such as <code>serviceAccounts</code>, <code>roles</code>, <code>roleBindings</code>....</p> <p>For this purpose, a small Helm chart can be created, which can either be generic or specific to our application.</p> <p>Such a chart is provided in the <code>charts/namespace/1.0.0</code> directory of our Git repository. It enables the creation of a  namespace with optional parameters for labels and annotations. Note the convention that allows us to version this chart.</p> <p>Here is a first version of the component using this chart, along with the associated <code>gitRepository</code>:</p> components/apps/_namespace-0.0.5.yaml <pre><code>gitRepositories:\n\n  - name: kad-infra-doc\n    watched: true\n    interval: 30s\n    protected: true\n    ref:\n      branch: main\n    url: https://github.com/kubotal/kad-infra-doc.git\n\ncomponents:\n\n- name: namespace\n  version: 0.0.5\n  source:\n    defaultVersion: 1.0.0\n    gitRepository:\n      name: kad-infra-doc\n      path: charts/namespace/{version}\n  parameters:\n    name: # TBD\n    labels: {}\n    annotations: {}\n  values: |\n    {{ toYaml .Parameters }\n</code></pre> <p>However, we won't be using this version. In fact, our Git repository is already referenced by FluxCD during the  bootstrap process. (Its definition is located in the file <code>clusters/kadtestX/flux/flux-system/gotk-sync</code>).</p> <p>As previously mentioned, a <code>gitRepository</code> consumes some resources, so it is advisable to avoid duplicating them.</p> <p>We will therefore use:</p> components/apps/namespace-0.1.0.yaml <pre><code>components:\n\n- name: namespace\n  version: 0.1.0\n  source:\n    defaultVersion: 1.0.0\n    gitRepository:\n      name: flux-system\n      namespace: flux-system\n      unmanaged: true\n      path: charts/namespace/{version}\n  parameters:\n    name: # TBD\n    labels: {}\n    annotations: {}\n  values: |\n    {{ toYaml .Parameters }}\n</code></pre> <p>Key points:</p> <ul> <li>The gitRepository object is not managed by KAD since it is under direct FluxCD control (<code>source.gitRepository.unmanaged: true</code>).</li> <li>The <code>namespace</code> of this gitRepository FluxCD resources must be specified (<code>source.gitRepository.namespace</code>).</li> <li>Versioning is managed by substituting the <code>{version}</code> token in the path if present.</li> </ul> <p>Another advantage of using this <code>gitRepository</code> is that its authentication is already managed by FluxCD.  In contrast, the first example only works if the Git repository is public (adding authentication information is  possible but is described later in this manual).</p>"},{"location":"getting-started/45-repository-source/#deployment_1","title":"Deployment","text":"<p>Here's a new version of the deployment, using this last component.</p> storehouse/redis-stack-2.yaml <pre><code>componentReleases:\n\n  - name: redis2-namespace\n    component:\n      name: namespace\n      version: 0.1.0\n      parameters:\n        name: redis2\n        labels:\n          my.company.com/project-name: redis\n          my.company.com/project-id: redis2\n    namespace: default\n\n  - name: redis2-redis\n    component:\n      name: redis\n      version: 0.1.0\n      parameters:\n        password: admin123\n        replicaCount: 1\n    namespace: redis2\n\n  - name: redis2-commander\n    component:\n      name: redis-commander\n      version: 0.1.0\n      parameters:\n        redis:\n          host: redis2-redis-master\n          password: admin123\n        hostname: redis2\n        tls: true\n    namespace: redis2\n</code></pre> <p>As previously mentioned, simply copy this file into the deployment directory of your cluster  (<code>cluster/kadTestX/deployments</code>), where it will automatically be processed by KAD, which will then handle its deployment.</p> <p>If everything goes well, three new <code>pods</code> should be created:</p> <pre><code>$ kubectl -n redis2 get pods\nNAME                                                READY   STATUS    RESTARTS      AGE\nredis2-commander-redis-commander-58f7b5c54f-r4xdq   1/1     Running   0             24h\nredis2-redis-master-0                               1/1     Running   0             24h\nredis2-redis-replicas-0                             1/1     Running   2 (15h ago)   24h\n</code></pre> <p>You can also validate that the labels have been correctly created for the namespace:</p> <pre><code>$ kubectl get namespace redis2 -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: redis2-namespace\n    meta.helm.sh/release-namespace: default\n  creationTimestamp: \"2025-01-02T09:32:18Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    helm.toolkit.fluxcd.io/name: redis2-namespace\n    helm.toolkit.fluxcd.io/namespace: flux-system\n    kubernetes.io/metadata.name: redis2\n    my.company.com/project-id: redis2\n    my.company.com/project-name: redis\n  name: redis2\n  resourceVersion: \"1743401\"\n  uid: 9523a057-81b5-4dcb-a2d1-983ca4afeff8\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n</code></pre>"},{"location":"getting-started/45-repository-source/#removal","title":"Removal","text":"<p>To delete our stack, you simply need to remove the corresponding file in the <code>deployment</code> directory. </p> <p>One of the consequences of explicitly creating the namespace as a <code>component</code> is that it will also be deleted.</p> <p>If this behavior is considered risky, several safeguards can be implemented. A dedicated chapter is provided to cover this aspect in detail.</p>"},{"location":"getting-started/50-dependencies/","title":"Dependencies","text":"<pre><code>$ kadcli k8s helmRelease list\nNAME              READY  STATUS                                                                                                                  AGE\n.....\nredis3-commander  False  dependency 'flux-system/redis3-redis' is not ready                                                                      2s\nredis3-namespace  True   Helm install succeeded for release default/redis3-namespace.v1 with chart namespace@1.0.0+e8788571eb8c                  3s\nredis3-redis      False  dependency 'flux-system/redis3-namespace' is not ready\n</code></pre> <pre><code>$ kadcli k8s helmRelease list\nNAME              READY    STATUS                                                                                                                  AGE\n.....\nredis3-commander  False    dependency 'flux-system/redis3-redis' is not ready                                                                      35s\nredis3-namespace  True     Helm install succeeded for release default/redis3-namespace.v1 with chart namespace@1.0.0+e8788571eb8c                  36s\nredis3-redis      Unknown  Running 'install' action with timeout of 3m0s\n</code></pre> <pre><code>$ kadcli k8s helmRelease list\nNAME              READY  STATUS                                                                                                                  AGE\n.....\nredis3-commander  True   Helm install succeeded for release redis3/redis3-commander.v1 with chart redis-commander@0.6.0+e8788571eb8c             4m38s\nredis3-namespace  True   Helm install succeeded for release default/redis3-namespace.v1 with chart namespace@1.0.0+e8788571eb8c                  4m39s\nredis3-redis      True   Helm install succeeded for release redis3/redis3-redis.v1 with chart redis@20.6.1+55659cf4e324\n</code></pre>"},{"location":"getting-started/55-stack/","title":"Application stack","text":"<p>https://joeferner.github.io/redis-commander/</p> <p>https://github.com/joeferner/redis-commander/tree/master/k8s</p>"},{"location":"guide/10-concepts-overview/","title":"Concepts and entities","text":"<p>Here is a short description of each object/entity defined by KAD.</p>"},{"location":"guide/10-concepts-overview/#component","title":"Component","text":"<p>A component is an application, a package aimed to be deployed on some cluster.</p> <p>Technically speaking, a component is a wrapper around a Helm chart.</p>"},{"location":"guide/10-concepts-overview/#componentreleases","title":"ComponentReleases","text":"<p>A ComponentRelease is the instantiation of a Component on a target cluster.</p> <p>A ComponentRelease is configured with a set of variables, provided as parameters and retrieved from the context.</p> <p>Technically speaking, a 'componentRelease' will generate one fluxCD object of kind 'HelmRelease'.</p>"},{"location":"guide/10-concepts-overview/#context","title":"Context","text":"<p>The Context is a set of variables bound to the target cluster.</p> <p>The context can be defined through a hierarchy of embedding entities. For example, a cluster belong to an infrastructure, which belong to an infrastructure type (OpenStack, AWS, GCP,...) which belong to an organization.</p> <p>Each level can enrich the Context with its own specific variable.</p> <p>It is up to the KAD Administrator to define this hierarchy.</p> <p>A good practice is to populate the context with only cluster specifics variables. Variables sp\u00e9cific to a deployment, a team, a project, etc... are better to be provided as parameters on releases.</p>"},{"location":"guide/10-concepts-overview/#source","title":"Source","text":"<p>A source is a repository of Helm chart. It is part of the component definition. </p> <p>Technically speaking a source is a FluxCD object of kind 'HelmRepository', 'GitRepository' or OCIRepository.</p>"},{"location":"guide/10-concepts-overview/#templating","title":"Templating","text":"<p>KAD make heavy usage of templating, at several level.</p> <p>A template is a text file with special tags allowing variable insertion. Such variables are provided by a data structure called a 'data model'.</p> <p>Template rendering is the action of merging the template with a data model, producing a new set of values.</p> <p>KAD use the same template engine (thus the same syntax) than the Helm templating system. So, you can refer to the Helm documentation for more information.</p>"},{"location":"guide/10-concepts-overview/#template","title":"Template","text":"<p>There is 'Template' KAD object. It allow to dynamically create new KAD objects, A typical use case is the creation of some  'application stack' by grouping a set of components, and then handling them as a single entity.</p>"},{"location":"guide/10-concepts-overview/#templaterelease","title":"TemplateRelease","text":"<p>A 'TemplateRelease' is the instantiation of a 'template' object, with a specific set of parameters.</p>"},{"location":"guide/10-concepts-overview/#kadfiles","title":"KadFiles","text":"<p>The entities described above are defined as object in yaml. Theses objects are defined in one or several yaml files, named kadFiles in the following.</p> <p>These kadFiles are stored in one or several Git repositories, referenced a FluxCD source. Theses sources are called 'Primary source' to distinguish from 'standard' sources, used by the Component objects.</p> <p>Primary sources and related kadFiles list are part of the initial configuration of the KAD system.</p> <p>In most cases, there will be a single primary source, which is the 'flux-system' source, the root of FluxCD configuration.</p> <p>All object are consolidated by KAD in an internal data structure called the 'referential' in this documentation.</p>"},{"location":"reference/component-release/","title":"ComponentReleases","text":""},{"location":"reference/component-release/#properties","title":"Properties","text":""},{"location":"reference/components/","title":"Components","text":""},{"location":"reference/components/#overview","title":"Overview","text":"podinfo-1.0.0.yaml <p>``` {.yaml .copy} components:   - name: podinfo     version: 1.0.0</p> <p>``` {.yaml} Context:   ....</p> <p>```</p>"},{"location":"reference/components/#properties","title":"properties","text":""},{"location":"reference/components/#name","title":"name","text":""},{"location":"reference/components/#version","title":"version","text":"<p>The version of the component. We strongly suggest to use semantic versioning: MAJOR.MINOR.PATCH</p>"},{"location":"reference/components/#catalogs","title":"catalogs","text":""},{"location":"reference/components/#usage","title":"usage","text":""},{"location":"reference/components/#config","title":"config","text":""},{"location":"reference/components/#suspended","title":"suspended","text":""},{"location":"reference/components/#protected","title":"protected","text":""},{"location":"reference/components/#sourceallowedversions","title":"source.allowedVersions","text":""},{"location":"reference/components/#sourcedefaultversion","title":"source.defaultVersion","text":""},{"location":"reference/components/#sourcegitrepository","title":"source.gitRepository","text":""},{"location":"reference/components/#sourceocirepository","title":"source.ociRepository","text":""},{"location":"reference/components/#sourcehelmrepository","title":"source.helmRepository","text":""},{"location":"reference/components/#parameters","title":"parameters","text":""},{"location":"reference/components/#parametersschema","title":"parametersSchema","text":""},{"location":"reference/components/#contextschema","title":"contextSchema","text":""},{"location":"reference/components/#values","title":"values","text":""},{"location":"reference/components/#allowvalues","title":"allowValues","text":""},{"location":"reference/components/#allowcreatenamespace","title":"allowCreateNamespace","text":""},{"location":"reference/components/#roles","title":"roles","text":""},{"location":"reference/components/#dependson","title":"dependsOn","text":""},{"location":"reference/components/#gitrepository","title":"gitRepository","text":""},{"location":"reference/components/#sourcegitrepositoryname","title":"source.gitRepository.name","text":""},{"location":"reference/components/#sourcegitrepositorypath","title":"source.gitRepository.path","text":""},{"location":"reference/components/#sourcegitrepositorypath_1","title":"source.gitRepository.path","text":""},{"location":"reference/components/#sourcegitrepositoryunmanaged","title":"source.gitRepository.unmanaged","text":""},{"location":"reference/components/#sourcegitrepositorynamespace","title":"source.gitRepository.namespace","text":""},{"location":"reference/components/#ocirepository","title":"ociRepository","text":""},{"location":"reference/components/#sourceocirepositoryurl","title":"source.ociRepository.url","text":""},{"location":"reference/components/#sourceocirepositoryinsecure","title":"source.ociRepository.insecure","text":""},{"location":"reference/components/#sourceocirepositoryinterval","title":"source.ociRepository.interval","text":""},{"location":"reference/components/#sourceocirepositorysecretref","title":"source.ociRepository.secretRef","text":""},{"location":"reference/components/#sourceocirepositorycertsecretref","title":"source.ociRepository.certSecretRef","text":""},{"location":"reference/components/#helmrepository","title":"helmRepository","text":""},{"location":"reference/components/#sourcehelmrepositoryurl","title":"source.helmRepository.url","text":""},{"location":"reference/components/#sourcehelmrepositorychart","title":"source.helmRepository.chart","text":""},{"location":"reference/components/#sourcehelmrepositoryinterval","title":"source.helmRepository.interval","text":""},{"location":"reference/components/#sourcehelmrepositorysecretref","title":"source.helmRepository.secretRef","text":""},{"location":"reference/components/#sourcehelmrepositorycertsecretref","title":"source.helmRepository.certSecretRef","text":""}]}